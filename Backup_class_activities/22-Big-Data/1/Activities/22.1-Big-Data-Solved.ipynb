{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@channel **Hello Everyone,**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2024-02-06 `22.1-Big-Data-Introduction to Big Data`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of you did fantastic working through the ML notebooks. I really hope all of you continue that journey as ML is a very exciting career path.  Please let me know if you have any further questions about ML.\n",
    " \n",
    "Alright gang, you have arrived at the last week of \"official\" lecture **BIG DATA**!!!  You will find when you hit the streets with all of your new found knowledge, that data just doesn't always fit on your laptop, or even that big server the IT team spent `$250k` for.  That combined with the straight up speed of incoming data, we needed more.. that's where **Big Data** comes in.  \n",
    "\n",
    "We will be using [Google Colab](https://colab.research.google.com/) this week, so please verify you can access it (plan to come early to class if you cannot)\n",
    "\n",
    "Here are some resources to prep you for day 1 of Big Data:\n",
    "\n",
    "* [Nice easy explanation of base Hadoop](https://www.hadoop360.datasciencecentral.com/blog/what-is-hadoop-an-easy-explanation-for-absolutely-anyone)\n",
    "* [Spark description (I will be adding a LOT more to this)](https://www.bernardmarr.com/default.asp?contentID=1079)\n",
    "* [Big Data Use Cases](https://www.datamation.com/big-data/big-data-use-cases.html)\n",
    "* [Google Colab](https://colab.research.google.com/)\n",
    "* [Here's a great video explaining HDFS for people who like Legos](https://www.youtube.com/watch?v=4Gfl0WuONMY)\n",
    "* [mrjob](https://github.com/Yelp/mrjob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "\n",
    "* Identify the parts of the Hadoop ecosystem.\n",
    "* Write a Python script that implements the MapReduce programming model.\n",
    "* Identify the differences between the Hadoop and Spark environments.\n",
    "* Create a DataFrame by using PySpark.\n",
    "* Filter and order a DataFrame by using Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Presentation**\n",
    "* [22.1-Introduction to Big Data](https://ucb.bootcampcontent.com/UCB-Coding-Bootcamp/UCB-VIRT-DATA-PT-11-2022-U-LOLC/-/blob/main/slides/Data-22.1-Intro_to_Big_Data.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install**\n",
    "* [Download WinRAR](https://www.win-rar.com/start.html?&L=0)\n",
    "* [Download JDK Development Kit](https://www.oracle.com/java/technologies/downloads/)\n",
    "* [Download Apache Spark](https://spark.apache.org/downloads.html)\n",
    "   - Extract Spark to a folder, example `C:/spark-3.4.0-bin-hadoop3`\n",
    "   - Restart your `terminal`/`git bash`\n",
    "   - Add to your env `HADOOP_HOME=C:\\spark-3.4.0-bin-hadoop3`\n",
    "   - Add to yor path `%HADOOP_HOME%\\bin`\n",
    "   - Copy [winutils](https://github.com/steveloughran/winutils/tree/master/hadoop-3.0.0/bin) to `C:\\spark-3.4.0-bin-hadoop3\\bin`\n",
    "   \n",
    "\n",
    "```\n",
    "pip install pyspark findspark\n",
    "pip install MRJob\n",
    "pip install plotly\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best wishes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* **Instructor Do: Welcome Class (Slides 1–2) (5 mins)**\n",
    "* **Instructor Do: Intro to Big Data (Slides 3–12) (10 mins)**\n",
    "* **Instructor Do: Intro to MapReduce with mrjob (Slides 13-24) (10 mins)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark findspark\n",
    "# !pip install MRJob\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.01 Students Do: MapReduce Use Cases (10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Use Cases\n",
    "\n",
    "In this activity, you’ll find common use cases for MapReduce. The ability to research and learn about common use cases is an important skill to have in your career.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Using a search engine, search for two or three use cases of MapReduce. Be prepared to share your findings with the class. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Use Cases Solution\n",
    "\n",
    "* Identifying potential security breaches. That is, using MapReduce to analyze and identify internal messages, emails, or server connection attempts that seem suspicious.\n",
    "\n",
    "* Analyzing social media data to determine how people feel about your company.\n",
    "\n",
    "* Analyzing log files from either a computer or a software component that produces lots of logs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.02 Everyone Do: MRJob with a CSV File (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everyone Do\n",
    "\n",
    "In this activity you'll code along with the instructor to implement MapReduce using MRJob.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "python <file_name> <input_File>\n",
    "# locally\n",
    "python hot_solution.py ../Resources/austin_weather_2017.csv\n",
    "# on EMR\n",
    "python hot_solution.py ../Resources/austin_weather_2017.csv -r emr\n",
    "# on Dataproc\n",
    "python hot_solution.py ../Resources/austin_weather_2017.csv -r dataproc\n",
    "# on your Hadoop cluster\n",
    "python hot.py ../Resources/austin_weather_2017.csv -r hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Find the number of hot days in Austin for 2017\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class Hot_Days(MRJob):\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        (station, name, state, date, snow, tmax, tmin) = line.split(\",\")\n",
    "        if tmax and int(tmax) >= 100:\n",
    "            yield name, 1\n",
    "\n",
    "    def reducer(self, name, hot):\n",
    "        yield name, sum(hot)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Hot_Days.run()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.03 Students Do: Snow in Austin (14 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow in Austin\n",
    "\n",
    "In this activity, you’ll use MRJob to print out the days when it snowed in Austin, Texas.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Review the `austin_weather_2017.csv` file.\n",
    "\n",
    "2. Import MRJob.\n",
    "\n",
    "3. Create a new class that will contain your `mapper` and `reducer` functions.\n",
    "\n",
    "4. Create `mapper` and `reducer` functions that return the days when it snowed in Austin, Texas. \n",
    "\n",
    "## Bonus\n",
    "\n",
    "Calculate the maximum amount of snow per date.\n",
    "\n",
    "**Hint:** Use the `max` function to reduce the values for a date to a single value.\n",
    "\n",
    "## References\n",
    "\n",
    "National Centers for Environmental Information: National Oceanic and Atmospheric Administration. Daily Summaries Station Details. Austin 14.7 WSW, TX  US. Retrieved from [https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:US1TXHYS059/detail](https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:US1TXHYS059/detail).\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "\"\"\"\n",
    "Which days had snow?\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class Snow_days(MRJob):\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        (station, name, state, date, snow, tmax, tmin) = line.split(\",\")\n",
    "        if snow and float(snow) > 0:\n",
    "            yield date, 1\n",
    "\n",
    "    def reducer(self, date, snow):\n",
    "        yield date, max(snow)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Snow_days.run()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus\n",
    "```python\n",
    "\"\"\"\n",
    "What was the max amount of snow per date?\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class Snow_days(MRJob):\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        (station, name, state, date, snow, tmax, tmin) = line.split(\",\")\n",
    "        if snow and float(snow) > 0:\n",
    "            yield date, snow\n",
    "\n",
    "    def reducer(self, date, snow):\n",
    "        yield date, max(snow)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Snow_days.run()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```python\n",
    "python hot.py ../Resources/austin_weather_2017.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.04.1 Instructor Do: Spark Overview (Slides 25–38) (10 mins)\n",
    "### 1.04.2 Instructor Do: Setup Google Colab (5 mins)\n",
    "### 1.04.3 Instructor Do: PySpark DataFrame Basics (10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark_dataframe_basics_solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vv16HppWUeva"
   },
   "source": [
    "## Dataframe Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPRmMt8FVirP"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "t3cCpP6GAyf5",
    "outputId": "f4ea8f58-fc1a-4a50-bdc9-987cc6818029",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame manually\n",
    "dataframe = spark.createDataFrame([\n",
    "    (0, \"Here is our DataFrame\"),\n",
    "    (1, \"We are making one from scratch\"),\n",
    "    (2, \"This will look very similar to a Pandas DataFrame\")\n",
    "], [\"id\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "t3cCpP6GAyf5",
    "outputId": "f4ea8f58-fc1a-4a50-bdc9-987cc6818029",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------+\n",
      "|id |words                                            |\n",
      "+---+-------------------------------------------------+\n",
      "|0  |Here is our DataFrame                            |\n",
      "|1  |We are making one from scratch                   |\n",
      "|2  |This will look very similar to a Pandas DataFrame|\n",
      "+---+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "QvwWeOR2Uev-",
    "outputId": "f9b9c728-da0a-4410-f308-11dd6f195555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   food|price|\n",
      "+-------+-----+\n",
      "|  pizza|    0|\n",
      "|  sushi|   12|\n",
      "|chinese|   10|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/1/food.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"food.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "uwIhrrWlUevj",
    "outputId": "dea2bf36-984d-421e-e97f-336114155ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- food: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print our schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vks4BtyYUevo",
    "outputId": "91e0084a-f807-4e76-893d-26a350cffe8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food', 'price']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HIEN0qoVUevs",
    "outputId": "fc094005-40e2-44bd-c823-da336b99948f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, food: string, price: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe our data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3yzsJJbUevw"
   },
   "outputs": [],
   "source": [
    "# Import struct fields that we can use\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K_kPLRBMUev0",
    "outputId": "6391dbcd-5e58-4895-c1da-e3f35d53a92b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField('food', StringType(), True),\n",
       " StructField('price', IntegerType(), True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we need to create the list of struct fields\n",
    "schema = [StructField(\"food\", StringType(), True), StructField(\"price\", IntegerType(), True),]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G8MrlcEgUev7",
    "outputId": "6a917d30-4ebd-4a78-85ca-eea9f4ffee15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('food', StringType(), True), StructField('price', IntegerType(), True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass in our fields\n",
    "final = StructType(fields=schema)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   food|price|\n",
      "+-------+-----+\n",
      "|  pizza|    0|\n",
      "|  sushi|   12|\n",
      "|chinese|   10|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read our data with our new schema\n",
    "dataframe = spark.read.csv(SparkFiles.get(\"food.csv\"), schema=final, sep=\",\", header=True)\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "YD1CT3HoUewC",
    "outputId": "de0e9a87-2c3e-46bc-9d1e-dc9b3b13431d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- food: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print it out\n",
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vq-kfT6yUewG"
   },
   "source": [
    "### Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "leLzlTgfUewL",
    "outputId": "43b5682c-7fac-4bb1-a2fe-f9f94538e0c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'price'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ut-wveumUewQ",
    "outputId": "ef71fe98-2c2c-4cb7-a280-71dd37740486"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataframe['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iZi9NQGwUewU",
    "outputId": "f5257e1a-3c0b-4ae4-bd2e-d9d0206d8443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[price: int]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.select('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CyUeV2G5UewX",
    "outputId": "bad3420d-51e8-42fb-b7d8-ac4788aec3dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataframe.select('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "jMd-MI_yUewj",
    "outputId": "18a79550-230b-48cb-cac4-46d5715ae041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|    0|\n",
      "|   12|\n",
      "|   10|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.select('price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uO2MMkR1Uewm"
   },
   "source": [
    "### Manipulating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "UuzYlpNwUewo",
    "outputId": "d29365a6-34d9-457e-b188-47493d076a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+\n",
      "|   food|price|newprice|\n",
      "+-------+-----+--------+\n",
      "|  pizza|    0|       0|\n",
      "|  sushi|   12|      12|\n",
      "|chinese|   10|      10|\n",
      "+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new column\n",
    "dataframe.withColumn('newprice', dataframe['price']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "1nMI3hXvUewt",
    "outputId": "4ffe760c-8c23-46fe-af79-e4cca20f4da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   food|newerprice|\n",
      "+-------+----------+\n",
      "|  pizza|         0|\n",
      "|  sushi|        12|\n",
      "|chinese|        10|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update column name\n",
    "dataframe.withColumnRenamed('price','newerprice').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "b4wkcAo7Uewz",
    "outputId": "94eab3b2-e29f-4abb-a327-4d8838c3a3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+\n",
      "|   food|price|doubleprice|\n",
      "+-------+-----+-----------+\n",
      "|  pizza|    0|          0|\n",
      "|  sushi|   12|         24|\n",
      "|chinese|   10|         20|\n",
      "+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Double the price\n",
    "dataframe.withColumn('doubleprice',dataframe['price']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "HCKNp0aEUew3",
    "outputId": "a0925899-7b0a-4eb0-c805-214a62345509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------+\n",
      "|   food|price|add_one_dollar|\n",
      "+-------+-----+--------------+\n",
      "|  pizza|    0|             1|\n",
      "|  sushi|   12|            13|\n",
      "|chinese|   10|            11|\n",
      "+-------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a dollar to the price\n",
    "dataframe.withColumn('add_one_dollar',dataframe['price']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "uppVIRZ3Uew8",
    "outputId": "69b30798-671a-4466-ba21-804b7282b2bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+\n",
      "|   food|price|half_price|\n",
      "+-------+-----+----------+\n",
      "|  pizza|    0|       0.0|\n",
      "|  sushi|   12|       6.0|\n",
      "|chinese|   10|       5.0|\n",
      "+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Half the price\n",
    "dataframe.withColumn('half_price',dataframe['price']/2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_p_Fft--Uew_",
    "outputId": "f1c2a79c-2847-4967-a974-7db4ff2afbeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(price=0), Row(price=12), Row(price=10)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collecting a column as a list\n",
    "dataframe.select(\"price\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dO0pdWQSUexD"
   },
   "source": [
    "# Converting PySpark DataFrame to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-0pF6RVUexE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = dataframe.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "9XMUMESlUexI",
    "outputId": "cb503614-560a-47d3-fee7-4c9639f9301a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pizza</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sushi</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chinese</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      food  price\n",
       "0    pizza      0\n",
       "1    sushi     12\n",
       "2  chinese     10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## spark_dataframe_basics_solution_colab.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv16HppWUeva"
   },
   "source": [
    "## Dataframe Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1sd9SzuUvob",
    "outputId": "310cc21f-43a3-491f-fe8f-bff76cd93ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease                         \n",
      "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
      "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
      "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
      "Get:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [988 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1,882 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,442 kB]\n",
      "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,381 kB]\n",
      "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,127 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,920 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,290 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,009 kB]\n",
      "Get:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\n",
      "Fetched 15.4 MB in 8s (1,997 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.3.1'\n",
    "spark_version = 'spark-3.3.2'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nPRmMt8FVirP"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3cCpP6GAyf5",
    "outputId": "516bf85e-7285-4258-f23a-5e1cbe5a132a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               words|\n",
      "+---+--------------------+\n",
      "|  0|Here is our DataF...|\n",
      "|  1|We are making one...|\n",
      "|  2|This will look ve...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame manually\n",
    "dataframe = spark.createDataFrame([\n",
    "    (0, \"Here is our DataFrame\"),\n",
    "    (1, \"We are making one from scratch\"),\n",
    "    (2, \"This will look very similar to a Pandas DataFrame\")\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVW7jBoKUeve",
    "outputId": "aeb45ae3-4d64-4cd6-adf0-51e23782ead7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   food|price|\n",
      "+-------+-----+\n",
      "|  pizza|    0|\n",
      "|  sushi|   12|\n",
      "|chinese|   10|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/1/food.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"food.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7AbwqP5jdJy",
    "outputId": "9621425f-420c-424b-9c62-414b3fc1ed0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- food: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print our schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwIhrrWlUevj",
    "outputId": "418f4f20-fe24-4e3d-853d-6c10b350c3c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food', 'price']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vks4BtyYUevo",
    "outputId": "45d5105b-d8aa-4aab-c2bb-ea4056e0b65c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food', 'price']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIEN0qoVUevs",
    "outputId": "fd0210bd-5a7f-4c20-ae75-8e55d16d285b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, food: string, price: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe our data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "U3yzsJJbUevw"
   },
   "outputs": [],
   "source": [
    "# Import struct fields that we can use\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_kPLRBMUev0",
    "outputId": "60749c41-bf80-47c7-f19a-7578b8029632"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField('food', StringType(), True),\n",
       " StructField('price', IntegerType(), True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we need to create the list of struct fields\n",
    "schema = [StructField(\"food\", StringType(), True), StructField(\"price\", IntegerType(), True),]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8MrlcEgUev7",
    "outputId": "0fab540e-30e1-409c-b4b0-c446197f1de3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('food', StringType(), True), StructField('price', IntegerType(), True)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass in our fields\n",
    "final = StructType(fields=schema)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvwWeOR2Uev-",
    "outputId": "42fba547-8561-4d01-994d-31df4ee1b087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   food|price|\n",
      "+-------+-----+\n",
      "|  pizza|    0|\n",
      "|  sushi|   12|\n",
      "|chinese|   10|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read our data with our new schema\n",
    "dataframe = spark.read.csv(SparkFiles.get(\"food.csv\"), schema=final, sep=\",\", header=True)\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YD1CT3HoUewC",
    "outputId": "f031c39a-e6b1-46b0-d3ff-6d545d0fe8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- food: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print it out\n",
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq-kfT6yUewG"
   },
   "source": [
    "### Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leLzlTgfUewL",
    "outputId": "63d33cd6-bd70-4bfe-dbca-6ce90aa1bd1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'price'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ut-wveumUewQ",
    "outputId": "9d7b8ad6-c090-4d12-e877-cbaaade7c785"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataframe['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZi9NQGwUewU",
    "outputId": "600515ec-d9d6-488e-afd7-5d1b29baecc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[price: int]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.select('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyUeV2G5UewX",
    "outputId": "26d4efff-0c77-4dda-e246-805057590398"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataframe.select('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMd-MI_yUewj",
    "outputId": "fda87124-e5bb-4cdf-f902-51dabe98f4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|    0|\n",
      "|   12|\n",
      "|   10|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.select('price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO2MMkR1Uewm"
   },
   "source": [
    "### Manipulating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UuzYlpNwUewo",
    "outputId": "c2820def-cb27-402b-bca9-14f6f32c25fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+\n",
      "|   food|price|newprice|\n",
      "+-------+-----+--------+\n",
      "|  pizza|    0|       0|\n",
      "|  sushi|   12|      12|\n",
      "|chinese|   10|      10|\n",
      "+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add new column\n",
    "dataframe.withColumn('newprice', dataframe['price']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nMI3hXvUewt",
    "outputId": "38f522f0-6bcb-4a6d-c293-32a6d793b593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   food|newerprice|\n",
      "+-------+----------+\n",
      "|  pizza|         0|\n",
      "|  sushi|        12|\n",
      "|chinese|        10|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update column name\n",
    "dataframe.withColumnRenamed('price','newerprice').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4wkcAo7Uewz",
    "outputId": "cf4160c2-bc17-40ab-e5e9-ad47b0fc1d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+\n",
      "|   food|price|doubleprice|\n",
      "+-------+-----+-----------+\n",
      "|  pizza|    0|          0|\n",
      "|  sushi|   12|         24|\n",
      "|chinese|   10|         20|\n",
      "+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Double the price\n",
    "dataframe.withColumn('doubleprice',dataframe['price']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HCKNp0aEUew3",
    "outputId": "e2bacdb0-3162-4fbb-e84f-4e93eba836ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------+\n",
      "|   food|price|add_one_dollar|\n",
      "+-------+-----+--------------+\n",
      "|  pizza|    0|             1|\n",
      "|  sushi|   12|            13|\n",
      "|chinese|   10|            11|\n",
      "+-------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a dollar to the price\n",
    "dataframe.withColumn('add_one_dollar',dataframe['price']+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uppVIRZ3Uew8",
    "outputId": "f5b4c008-487d-4f91-92b3-64e71b7b72a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+\n",
      "|   food|price|half_price|\n",
      "+-------+-----+----------+\n",
      "|  pizza|    0|       0.0|\n",
      "|  sushi|   12|       6.0|\n",
      "|chinese|   10|       5.0|\n",
      "+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Half the price\n",
    "dataframe.withColumn('half_price',dataframe['price']/2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_p_Fft--Uew_",
    "outputId": "054d28da-523c-433c-aff4-73c55cb95e28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(price=0), Row(price=12), Row(price=10)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collecting a column as a list\n",
    "dataframe.select(\"price\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO0pdWQSUexD"
   },
   "source": [
    "# Converting PySpark DataFrame to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Z-0pF6RVUexE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = dataframe.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "9XMUMESlUexI",
    "outputId": "aadb9171-6865-483d-a0d5-329422e49455"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ad67576f-c0ba-45a8-a243-88887a13daf1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pizza</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sushi</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chinese</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad67576f-c0ba-45a8-a243-88887a13daf1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-ad67576f-c0ba-45a8-a243-88887a13daf1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-ad67576f-c0ba-45a8-a243-88887a13daf1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      food  price\n",
       "0    pizza      0\n",
       "1    sushi     12\n",
       "2  chinese     10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BREAK (0:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.05 Students Do: Demographic DataFrame Basics (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographic DataFrame Basics\n",
    "\n",
    "In this activity, you will explore Spark DataFrames.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Follow the comments in the provided notebook to clean and display demographic data by using Spark DataFrames.\n",
    "\n",
    "Remember to consult the [documentation](http://spark.apache.org/docs/latest/api/python/index.html).\n",
    "\n",
    "## References\n",
    "\n",
    "Mockaroo, LLC. (2021). Realistic Data Generator. [https://www.mockaroo.com/](https://www.mockaroo.com/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XTc61chZZy5F"
   },
   "outputs": [],
   "source": [
    "# Import findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fz9FFdrHB6TO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/07 15:58:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Demographics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJ8AzEZdZvuX",
    "outputId": "c71e50af-abee-4e79-9516-a8ff1744cc8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+---+--------+---------+--------+--------------------+---------------+------+--------------------+\n",
      "| id|             name|age|height_m|weight_kg|children|          occupation|academic_degree|salary|            location|\n",
      "+---+-----------------+---+--------+---------+--------+--------------------+---------------+------+--------------------+\n",
      "|  1|    Glad Gavrieli| 38|    1.52|       74|       0|Computer Systems ...|       Bachelor|    78|           Louisiana|\n",
      "|  2|  Henrieta Fittes| 34|    1.72|       39|       4|             Teacher|         Master|    44|            Illinois|\n",
      "|  3|   Peyton Dulanty| 24|    1.80|       47|       5|Senior Quality En...|            PhD|    44|      North Carolina|\n",
      "|  4|     Denna Morgen| 48|    1.81|       71|       5|   Account Executive|         Master|    81|          California|\n",
      "|  5|    Camella Izaks| 34|    1.65|       60|       1|   Director of Sales|            PhD|    76|                Ohio|\n",
      "|  6|     Shara Esposi| 49|    1.50|       52|       2|     Sales Associate|         Master|    68|            Virginia|\n",
      "|  7|Saunderson Gudgen| 64|    1.93|       47|       0|Sales Representative|         Master|    85|            New York|\n",
      "|  8|Aldrich Rosendorf| 42|    1.77|       66|       0|        Developer II|            PhD|    84|District of Columbia|\n",
      "|  9| Edlin Washington| 50|    1.74|       52|       4|     Design Engineer|         Master|    88|              Oregon|\n",
      "| 10| Anders Penwright| 55|    1.98|       90|       2|     Project Manager|       Bachelor|   116|             Georgia|\n",
      "| 11|   Ruthie Cubbini| 67|    1.52|       55|       4|     Data Coordiator|            PhD|    51|District of Columbia|\n",
      "| 12|  Juliet Harbidge| 66|    1.61|       49|       2|             Actuary|            PhD|    66|         Mississippi|\n",
      "| 13|  Jeannine Kelner| 30|    1.80|       81|       1|       Programmer II|       Bachelor|    90|              Hawaii|\n",
      "| 14|  Roi Gristhwaite| 66|    1.86|       90|       1|    Product Engineer|       Bachelor|    40|           Tennessee|\n",
      "| 15|      Brant Stark| 58|    1.94|       53|       4|Sales Representative|       Bachelor|    96|      North Carolina|\n",
      "| 16|   Donny Rankling| 26|    1.51|       39|       1|Physical Therapy ...|       Bachelor|   116|            Virginia|\n",
      "| 17|     Dov Gavaghan| 27|    1.58|       68|       5|Computer Systems ...|       Bachelor|    74|                Utah|\n",
      "| 18|    Rosanne Wreak| 37|    1.76|       81|       2|Payment Adjustmen...|         Master|   103|             Florida|\n",
      "| 19|    Belva Spraggs| 50|    1.65|       80|       2|             Teacher|       Bachelor|    46|             Georgia|\n",
      "| 20|      Welch Lease| 21|    1.57|       79|       4|Mechanical System...|         Master|   114|       Massachusetts|\n",
      "+---+-----------------+---+--------+---------+--------+--------------------+---------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/1/demographics.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"demographics.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZFco7ARZvuf",
    "outputId": "3faf344c-829e-4a17-d796-de35bb57c517"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'age',\n",
       " 'height_m',\n",
       " 'weight_kg',\n",
       " 'children',\n",
       " 'occupation',\n",
       " 'academic_degree',\n",
       " 'salary',\n",
       " 'location']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wdj33caiZvuj",
    "outputId": "e2f17d69-76d2-4f61-c12c-53d59969333b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+---+--------+---------+--------+--------------------+---------------+------+--------------------+\n",
      "| id|             name|age|height_m|weight_kg|children|          occupation|academic_degree|salary|            location|\n",
      "+---+-----------------+---+--------+---------+--------+--------------------+---------------+------+--------------------+\n",
      "|  1|    Glad Gavrieli| 38|    1.52|       74|       0|Computer Systems ...|       Bachelor|    78|           Louisiana|\n",
      "|  2|  Henrieta Fittes| 34|    1.72|       39|       4|             Teacher|         Master|    44|            Illinois|\n",
      "|  3|   Peyton Dulanty| 24|    1.80|       47|       5|Senior Quality En...|            PhD|    44|      North Carolina|\n",
      "|  4|     Denna Morgen| 48|    1.81|       71|       5|   Account Executive|         Master|    81|          California|\n",
      "|  5|    Camella Izaks| 34|    1.65|       60|       1|   Director of Sales|            PhD|    76|                Ohio|\n",
      "|  6|     Shara Esposi| 49|    1.50|       52|       2|     Sales Associate|         Master|    68|            Virginia|\n",
      "|  7|Saunderson Gudgen| 64|    1.93|       47|       0|Sales Representative|         Master|    85|            New York|\n",
      "|  8|Aldrich Rosendorf| 42|    1.77|       66|       0|        Developer II|            PhD|    84|District of Columbia|\n",
      "|  9| Edlin Washington| 50|    1.74|       52|       4|     Design Engineer|         Master|    88|              Oregon|\n",
      "| 10| Anders Penwright| 55|    1.98|       90|       2|     Project Manager|       Bachelor|   116|             Georgia|\n",
      "+---+-----------------+---+--------+---------+--------+--------------------+---------------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the first 10 rows\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CifAfVSzZvun",
    "outputId": "91426d66-47d7-4ba3-d1e9-ffb3bc9679f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+\n",
      "|summary|               age|           height_m|         weight_kg|\n",
      "+-------+------------------+-------------------+------------------+\n",
      "|  count|              1000|               1000|              1000|\n",
      "|   mean|            42.961|  1.750380000000002|            64.308|\n",
      "| stddev|14.166869067623207|0.14157152997018183|15.840077147667067|\n",
      "|    min|                18|               1.50|                38|\n",
      "|    max|                67|               2.00|                90|\n",
      "+-------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the age, height_meter, and weight_kg columns and use describe to show the summary statistics\n",
    "df.select([\"age\", \"height_m\", \"weight_kg\"]).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHyNTIS7Zvur",
    "outputId": "c4287191-9ecc-4e26-edae-939d15af5cf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- height_m: string (nullable = true)\n",
      " |-- weight_kg: string (nullable = true)\n",
      " |-- children: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- academic_degree: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema to see the types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAmRujSRZvuv",
    "outputId": "7d66af11-cebc-4db4-99bc-7b4ccad2c69b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Salary (1k)|\n",
      "+-----------+\n",
      "|         78|\n",
      "|         44|\n",
      "|         44|\n",
      "|         81|\n",
      "|         76|\n",
      "|         68|\n",
      "|         85|\n",
      "|         84|\n",
      "|         88|\n",
      "|        116|\n",
      "|         51|\n",
      "|         66|\n",
      "|         90|\n",
      "|         40|\n",
      "|         96|\n",
      "|        116|\n",
      "|         74|\n",
      "|        103|\n",
      "|         46|\n",
      "|        114|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the Salary column to `Salary (1k)` and show only this new column\n",
    "df = df.withColumnRenamed('Salary', 'Salary (1k)')\n",
    "df.select(\"Salary (1k)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYoi9OvwZvu0",
    "outputId": "cf107e89-b5f8-4fe6-b854-0bc9d20d18d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|  Salary|Salary (1k)|\n",
      "+--------+-----------+\n",
      "| 78000.0|         78|\n",
      "| 44000.0|         44|\n",
      "| 44000.0|         44|\n",
      "| 81000.0|         81|\n",
      "| 76000.0|         76|\n",
      "| 68000.0|         68|\n",
      "| 85000.0|         85|\n",
      "| 84000.0|         84|\n",
      "| 88000.0|         88|\n",
      "|116000.0|        116|\n",
      "| 51000.0|         51|\n",
      "| 66000.0|         66|\n",
      "| 90000.0|         90|\n",
      "| 40000.0|         40|\n",
      "| 96000.0|         96|\n",
      "|116000.0|        116|\n",
      "| 74000.0|         74|\n",
      "|103000.0|        103|\n",
      "| 46000.0|         46|\n",
      "|114000.0|        114|\n",
      "+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column called `Salary` where the values are the `Salary (1k)` * 1000\n",
    "# Show the columns `Salary` and `Salary (1k)`\n",
    "df = df.withColumn(\"Salary\", df[\"Salary (1k)\"] * 1000)\n",
    "df.select([\"Salary\", \"Salary (1k)\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.06 Instructor Do: PySpark DataFrame Filtering (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Demo\n",
    "\n",
    "## References\n",
    "\n",
    "Google Research Datasets repository (2021). Data modified from \"Nutrition5k Dataset\". Retrieved from GitHub [https://github.com/google-research-datasets/Nutrition5k\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nx-RvtVhbIdh"
   },
   "outputs": [],
   "source": [
    "# Import findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "7RLRYd3QP2KF"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkFunctions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HZ7JR0Ia-QC",
    "outputId": "6966e0a0-d90c-4dfa-ee04-7db639a7201b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|        dish_id|total_calories|total_mass|total_fat|total_carb|total_protein|         ingredients|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|dish_1561662216|    300.794281|193.000000|12.387489| 28.218290|    18.633970|soy sauce; garlic...|\n",
      "|dish_1562688426|    137.569992| 88.000000| 8.256000|  5.190000|    10.297000|roasted potatoes;...|\n",
      "|dish_1561662054|    419.438782|292.000000|23.838249| 26.351543|    25.910593|pepper; white ric...|\n",
      "|dish_1562008979|    382.936646|290.000000|22.224644| 10.173570|    35.345387|jalapenos; lemon ...|\n",
      "|dish_1560455030|     20.590000|103.000000| 0.148000|  4.625000|     0.956000|cherry tomatoes; ...|\n",
      "|dish_1558372433|     74.360001|143.000000| 0.286000|  0.429000|    20.020000|          deprecated|\n",
      "|dish_1563379132|    232.050003|119.000000|14.280000| 14.280000|    10.591001|         chilaquiles|\n",
      "|dish_1565640549|     45.482903|139.000000| 1.568471|  7.043886|     2.641478|tomatoes; cilantr...|\n",
      "|dish_1563207364|    309.269989|271.000000|13.774000| 30.657000|    15.010000|scrambled eggs; y...|\n",
      "|dish_1561575474|    120.058434|183.000000| 4.966118| 17.412746|     2.990431|salt; eggplant; r...|\n",
      "|dish_1550795690|     68.119995|131.000000| 0.262000| 18.340000|     0.393000|               apple|\n",
      "|dish_1563216717|    246.007996|332.000000| 5.636517| 16.284782|    33.068710|garlic; squash; c...|\n",
      "|dish_1565972591|    195.199997|122.000000|12.200000|  3.660000|    17.080000|chicken apple sau...|\n",
      "|dish_1568649312|     44.459999| 78.000000| 0.234000| 10.920000|     0.546000|             berries|\n",
      "|dish_1550876012|    253.520004|207.000000| 4.455999| 12.460000|    38.608002|chicken; cantalou...|\n",
      "|dish_1551565034|      7.790000| 19.000000| 0.038000|  1.900000|     0.171000|              carrot|\n",
      "|dish_1550860747|     39.500000| 79.000000| 0.079000| 10.270000|     0.395000|           pineapple|\n",
      "|dish_1566245398|    506.355377|230.000000|31.183908|  4.258222|    49.023197|olive oil; lime; ...|\n",
      "|dish_1563381680|     32.459999|  6.000000| 2.520000|  0.084000|     2.220000|               bacon|\n",
      "|dish_1562183096|     41.188980| 95.000000| 2.571043|  3.280260|     2.804970|pepper; chard; bl...|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "url =\"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/1/nutrition.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"nutrition.csv\"), sep=\",\", header=True, ignoreLeadingWhiteSpace=True) #Observe the need to use ignoreLeadingWhiteSpace=True, otherwise a leading whitespace will appear in the column names\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Va2Sjj_oa-QM",
    "outputId": "8098afe4-9611-4874-888b-06063d9897bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----------+---------+----------+-------------+-------------+\n",
      "|        dish_id|total_calories|total_mass|total_fat|total_carb|total_protein|  ingredients|\n",
      "+---------------+--------------+----------+---------+----------+-------------+-------------+\n",
      "|dish_1557861216|      0.000000|  1.000000| 0.000000|  0.000000|     0.000000|   plate only|\n",
      "|dish_1556575700|      0.000000| 86.000000| 0.000000|  0.000000|     0.000000|   plate only|\n",
      "|dish_1558461431|      1.150000|  5.000000| 0.020000|  0.180000|     0.145000|spinach (raw)|\n",
      "|dish_1558460205|      1.840000|  8.000000| 0.032000|  0.288000|     0.232000|spinach (raw)|\n",
      "|dish_1551135590|     10.000000| 25.000000| 0.050000|  2.250000|     0.500000| bell peppers|\n",
      "+---------------+--------------+----------+---------+----------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order a DataFrame by ascending values\n",
    "df.orderBy(df[\"total_calories\"].asc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJhqS-3qQFRI",
    "outputId": "fae5e5a4-dbc5-4ee3-80e8-c0cec0ac2e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|        dish_id|total_calories|total_mass|total_fat|total_carb|total_protein|         ingredients|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|dish_1566931674|    990.989014|531.000000|76.803001| 23.140001|    58.401997|chicken; mixed gr...|\n",
      "|dish_1563476408|    990.400024|513.000000|47.925026| 55.908291|    79.199821|salmon; garlic; s...|\n",
      "|dish_1559678104|     99.962006|135.000000| 7.348568|  7.806039|     2.654628|cucumbers; olive ...|\n",
      "|dish_1558721434|     99.900002|350.000000| 0.778000| 22.713001|     3.759000|cherry tomatoes; ...|\n",
      "|dish_1565981802|     99.754425|152.000000| 5.596065|  6.631088|     8.817584|carrot; salt; tof...|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order a DataFrame by descending values\n",
    "df.orderBy(df[\"total_calories\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pREWDG0a-QQ",
    "outputId": "0cd66e2e-5cfb-41ce-b9f3-95381829170d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|avg(total_calories)|\n",
      "+-------------------+\n",
      "| 223.98083459731635|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import average function\n",
    "from pyspark.sql.functions import avg\n",
    "df.select(avg(\"total_calories\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iy7YqMo7a-QU",
    "outputId": "0d6e8742-6ae7-43c6-cb59-35f9da06d1ff",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|        dish_id|total_calories|total_mass|total_fat|total_carb|total_protein|         ingredients|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|dish_1562688426|    137.569992| 88.000000| 8.256000|  5.190000|    10.297000|roasted potatoes;...|\n",
      "|dish_1560455030|     20.590000|103.000000| 0.148000|  4.625000|     0.956000|cherry tomatoes; ...|\n",
      "|dish_1558372433|     74.360001|143.000000| 0.286000|  0.429000|    20.020000|          deprecated|\n",
      "|dish_1565640549|     45.482903|139.000000| 1.568471|  7.043886|     2.641478|tomatoes; cilantr...|\n",
      "|dish_1561575474|    120.058434|183.000000| 4.966118| 17.412746|     2.990431|salt; eggplant; r...|\n",
      "|dish_1550795690|     68.119995|131.000000| 0.262000| 18.340000|     0.393000|               apple|\n",
      "|dish_1565972591|    195.199997|122.000000|12.200000|  3.660000|    17.080000|chicken apple sau...|\n",
      "|dish_1568649312|     44.459999| 78.000000| 0.234000| 10.920000|     0.546000|             berries|\n",
      "|dish_1551565034|      7.790000| 19.000000| 0.038000|  1.900000|     0.171000|              carrot|\n",
      "|dish_1550860747|     39.500000| 79.000000| 0.079000| 10.270000|     0.395000|           pineapple|\n",
      "|dish_1563381680|     32.459999|  6.000000| 2.520000|  0.084000|     2.220000|               bacon|\n",
      "|dish_1562183096|     41.188980| 95.000000| 2.571043|  3.280260|     2.804970|pepper; chard; bl...|\n",
      "|dish_1550778583|     67.759995| 88.000000| 0.088000| 14.960000|     1.760000|            potatoes|\n",
      "|dish_1566417398|     56.892494| 46.000000| 5.357862|  2.088042|     0.792647|mustard; vinegar;...|\n",
      "|dish_1568144828|     76.748177|152.000000| 4.168533| 10.369692|     1.329856|salt; pepper; car...|\n",
      "|dish_1550708440|     37.500000| 75.000000| 0.075000|  9.750000|     0.375000|           pineapple|\n",
      "|dish_1551307090|    171.550003| 52.000000|11.686000| 11.480000|     6.152000|potato chips; bac...|\n",
      "|dish_1562862493|    198.320007|134.000000|14.740000|  2.144000|    13.400001|      scrambled eggs|\n",
      "|dish_1558549008|     20.700001|115.000000| 0.230000|  4.485000|     1.035000|     cherry tomatoes|\n",
      "|dish_1551226363|     17.150000| 49.000000| 0.196000|  3.430000|     1.176000|            broccoli|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using filter\n",
    "df.filter(\"total_calories<200\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ww7emkaea-QY",
    "outputId": "7eedbf58-7f46-4fbd-c95f-027378661f0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+-------------+\n",
      "|total_mass|total_fat|total_carb|total_protein|\n",
      "+----------+---------+----------+-------------+\n",
      "| 88.000000| 8.256000|  5.190000|    10.297000|\n",
      "|103.000000| 0.148000|  4.625000|     0.956000|\n",
      "|143.000000| 0.286000|  0.429000|    20.020000|\n",
      "|139.000000| 1.568471|  7.043886|     2.641478|\n",
      "|183.000000| 4.966118| 17.412746|     2.990431|\n",
      "|131.000000| 0.262000| 18.340000|     0.393000|\n",
      "|122.000000|12.200000|  3.660000|    17.080000|\n",
      "| 78.000000| 0.234000| 10.920000|     0.546000|\n",
      "| 19.000000| 0.038000|  1.900000|     0.171000|\n",
      "| 79.000000| 0.079000| 10.270000|     0.395000|\n",
      "|  6.000000| 2.520000|  0.084000|     2.220000|\n",
      "| 95.000000| 2.571043|  3.280260|     2.804970|\n",
      "| 88.000000| 0.088000| 14.960000|     1.760000|\n",
      "| 46.000000| 5.357862|  2.088042|     0.792647|\n",
      "|152.000000| 4.168533| 10.369692|     1.329856|\n",
      "| 75.000000| 0.075000|  9.750000|     0.375000|\n",
      "| 52.000000|11.686000| 11.480000|     6.152000|\n",
      "|134.000000|14.740000|  2.144000|    13.400001|\n",
      "|115.000000| 0.230000|  4.485000|     1.035000|\n",
      "| 49.000000| 0.196000|  3.430000|     1.176000|\n",
      "+----------+---------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter by total_calories on certain columns\n",
    "df.filter(\"total_calories<200\").select(['total_mass','total_fat', 'total_carb','total_protein']).show() #excludes the last column (ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPuwAnZ_a-Qc"
   },
   "source": [
    "### Using Python Comparison Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WYZQGNIa-Qd",
    "outputId": "efb26da6-8e48-49c5-abe5-f12f39985e0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|        dish_id|total_calories|total_mass|total_fat|total_carb|total_protein|         ingredients|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|dish_1562688426|    137.569992| 88.000000| 8.256000|  5.190000|    10.297000|roasted potatoes;...|\n",
      "|dish_1560455030|     20.590000|103.000000| 0.148000|  4.625000|     0.956000|cherry tomatoes; ...|\n",
      "|dish_1558372433|     74.360001|143.000000| 0.286000|  0.429000|    20.020000|          deprecated|\n",
      "|dish_1565640549|     45.482903|139.000000| 1.568471|  7.043886|     2.641478|tomatoes; cilantr...|\n",
      "|dish_1561575474|    120.058434|183.000000| 4.966118| 17.412746|     2.990431|salt; eggplant; r...|\n",
      "|dish_1550795690|     68.119995|131.000000| 0.262000| 18.340000|     0.393000|               apple|\n",
      "|dish_1565972591|    195.199997|122.000000|12.200000|  3.660000|    17.080000|chicken apple sau...|\n",
      "|dish_1568649312|     44.459999| 78.000000| 0.234000| 10.920000|     0.546000|             berries|\n",
      "|dish_1551565034|      7.790000| 19.000000| 0.038000|  1.900000|     0.171000|              carrot|\n",
      "|dish_1550860747|     39.500000| 79.000000| 0.079000| 10.270000|     0.395000|           pineapple|\n",
      "|dish_1563381680|     32.459999|  6.000000| 2.520000|  0.084000|     2.220000|               bacon|\n",
      "|dish_1562183096|     41.188980| 95.000000| 2.571043|  3.280260|     2.804970|pepper; chard; bl...|\n",
      "|dish_1550778583|     67.759995| 88.000000| 0.088000| 14.960000|     1.760000|            potatoes|\n",
      "|dish_1566417398|     56.892494| 46.000000| 5.357862|  2.088042|     0.792647|mustard; vinegar;...|\n",
      "|dish_1568144828|     76.748177|152.000000| 4.168533| 10.369692|     1.329856|salt; pepper; car...|\n",
      "|dish_1550708440|     37.500000| 75.000000| 0.075000|  9.750000|     0.375000|           pineapple|\n",
      "|dish_1551307090|    171.550003| 52.000000|11.686000| 11.480000|     6.152000|potato chips; bac...|\n",
      "|dish_1562862493|    198.320007|134.000000|14.740000|  2.144000|    13.400001|      scrambled eggs|\n",
      "|dish_1558549008|     20.700001|115.000000| 0.230000|  4.485000|     1.035000|     cherry tomatoes|\n",
      "|dish_1551226363|     17.150000| 49.000000| 0.196000|  3.430000|     1.176000|            broccoli|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same results only this time using python\n",
    "# df.filter(\"total_calories=200\").show()\n",
    "df.filter(df[\"total_calories\"] < 200).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JK41k68oa-Qh",
    "outputId": "3304efb3-10d4-4469-bf87-ed34a8f684bb"
   },
   "outputs": [],
   "source": [
    "# Filter on the price column for items less than 200 dollars and greater than 80 dollars.\n",
    "# And &\n",
    "# OR |\n",
    "df3 = df.filter( (df[\"total_calories\"] < 200) & (df['total_mass'] > 80) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|        dish_id|total_calories|total_mass|total_fat|total_carb|total_protein|         ingredients|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "|dish_1562688426|    137.569992| 88.000000| 8.256000|  5.190000|    10.297000|roasted potatoes;...|\n",
      "|dish_1560455030|     20.590000|103.000000| 0.148000|  4.625000|     0.956000|cherry tomatoes; ...|\n",
      "|dish_1558372433|     74.360001|143.000000| 0.286000|  0.429000|    20.020000|          deprecated|\n",
      "|dish_1565640549|     45.482903|139.000000| 1.568471|  7.043886|     2.641478|tomatoes; cilantr...|\n",
      "|dish_1561575474|    120.058434|183.000000| 4.966118| 17.412746|     2.990431|salt; eggplant; r...|\n",
      "|dish_1550795690|     68.119995|131.000000| 0.262000| 18.340000|     0.393000|               apple|\n",
      "|dish_1565972591|    195.199997|122.000000|12.200000|  3.660000|    17.080000|chicken apple sau...|\n",
      "|dish_1562183096|     41.188980| 95.000000| 2.571043|  3.280260|     2.804970|pepper; chard; bl...|\n",
      "|dish_1550778583|     67.759995| 88.000000| 0.088000| 14.960000|     1.760000|            potatoes|\n",
      "|dish_1568144828|     76.748177|152.000000| 4.168533| 10.369692|     1.329856|salt; pepper; car...|\n",
      "|dish_1562862493|    198.320007|134.000000|14.740000|  2.144000|    13.400001|      scrambled eggs|\n",
      "|dish_1558549008|     20.700001|115.000000| 0.230000|  4.485000|     1.035000|     cherry tomatoes|\n",
      "|dish_1563822597|     59.400002|176.000000| 2.170667|  7.626667|     4.106667|broccoli; caesar ...|\n",
      "|dish_1565119669|     55.965000|229.000000| 1.030500| 10.842000|     3.272500|kale; zucchini; c...|\n",
      "|dish_1551233434|     27.230000|104.000000| 0.292000|  5.182000|     2.334000|mushroom; cucumbe...|\n",
      "|dish_1563393217|     49.230003|228.000000| 0.467667|  9.802668|     2.865667|zucchini; broccol...|\n",
      "|dish_1568144803|     66.128174| 93.000000| 4.050533|  8.068691|     0.798856|salt; pepper; car...|\n",
      "|dish_1551487158|     94.299995| 82.000000| 8.774000|  5.166000|     0.656000|              olives|\n",
      "|dish_1561997365|     98.951996|168.000000| 0.672000|  6.048000|    16.800001|              yogurt|\n",
      "|dish_1551395233|    139.410004|113.000000| 2.094000|  5.964000|    24.362000|chicken; corn; sh...|\n",
      "+---------------+--------------+----------+---------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmlEBelya-Qo",
    "outputId": "5dabd64b-2a22-4b65-eab9-5485d1f7eaf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----------+---------+----------+-------------+-----------+\n",
      "|        dish_id|total_calories|total_mass|total_fat|total_carb|total_protein|ingredients|\n",
      "+---------------+--------------+----------+---------+----------+-------------+-----------+\n",
      "|dish_1563381680|     32.459999|  6.000000| 2.520000|  0.084000|     2.220000|      bacon|\n",
      "|dish_1559319860|     70.330002| 13.000000| 5.460000|  0.182000|     4.810000|      bacon|\n",
      "|dish_1562086702|    178.529999| 33.000000|13.860000|  0.462000|    12.210000|      bacon|\n",
      "|dish_1551391710|    102.789993| 19.000000| 7.980000|  0.266000|     7.030000|      bacon|\n",
      "|dish_1564073860|    492.309998| 91.000000|38.219997|  1.274000|    33.670002|      bacon|\n",
      "|dish_1550776767|     81.149994| 15.000000| 6.300000|  0.210000|     5.550000|      bacon|\n",
      "|dish_1558032156|    140.660004| 26.000000|10.920000|  0.364000|     9.620000|      bacon|\n",
      "|dish_1551136683|     70.330002| 13.000000| 5.460000|  0.182000|     4.810000|      bacon|\n",
      "|dish_1550769483|    113.610001| 21.000000| 8.820000|  0.294000|     7.770000|      bacon|\n",
      "|dish_1565974375|    459.849976| 85.000000|35.700001|  1.190000|    31.450001|      bacon|\n",
      "|dish_1559059954|    108.199997| 20.000000| 8.400000|  0.280000|     7.400000|      bacon|\n",
      "|dish_1558031566|    140.660004| 26.000000|10.920000|  0.364000|     9.620000|      bacon|\n",
      "|dish_1551306860|     59.509998| 11.000000| 4.620000|  0.154000|     4.070000|      bacon|\n",
      "|dish_1551124637|    108.199997| 20.000000| 8.400000|  0.280000|     7.400000|      bacon|\n",
      "|dish_1562603536|     75.739998| 14.000000| 5.880000|  0.196000|     5.180000|      bacon|\n",
      "|dish_1559838214|     43.279999|  8.000000| 3.360000|  0.112000|     2.960000|      bacon|\n",
      "|dish_1558026714|    129.839996| 24.000000|10.080000|  0.336000|     8.880000|      bacon|\n",
      "|dish_1559232986|    189.349991| 35.000000|14.700000|  0.490000|    12.950000|      bacon|\n",
      "|dish_1561997248|    113.610001| 21.000000| 8.820000|  0.294000|     7.770000|      bacon|\n",
      "|dish_1550771191|     81.149994| 15.000000| 6.300000|  0.210000|     5.550000|      bacon|\n",
      "+---------------+--------------+----------+---------+----------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter on a specific value in a column. \n",
    "df.filter(df[\"ingredients\"] == \"bacon\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.07 Students Do: PySpark Demographic Filtering (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Demographic Filtering\n",
    "\n",
    "In this activity, you will summarize the data in various ways to answer demographic questions.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Use PySpark methods and the demographics dataset to answer the following questions:\n",
    "\n",
    "* Which occupation had the highest salary?\n",
    "* Which occupation had the lowest salary?\n",
    "* What is the mean salary of this dataset?\n",
    "* What is the `max` and `min` of the Salary column?\n",
    "* Which occupations have salaries above 80k? List all of them.\n",
    "\n",
    "## Bonus\n",
    "\n",
    "What is the average age and height of people for each academic degree type?\n",
    "\n",
    "**Hint:** You will need to use `groupBy` to answer this question.\n",
    "\n",
    "## References\n",
    "\n",
    "Mockaroo, LLC. (2021). Realistic Data Generator. [https://www.mockaroo.com/](https://www.mockaroo.com/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Wa-6wjlsRBIo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/07 16:18:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demographicsFilter\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcKW2EOginzm",
    "outputId": "932c0a46-d94e-478e-b478-02fdb1ee6031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+---+--------+---------+--------+--------------------+---------------+------+--------------------+\n",
      "| id|             name|age|height_m|weight_kg|children|          occupation|academic_degree|salary|            location|\n",
      "+---+-----------------+---+--------+---------+--------+--------------------+---------------+------+--------------------+\n",
      "|  1|    Glad Gavrieli| 38|    1.52|       74|       0|Computer Systems ...|       Bachelor|    78|           Louisiana|\n",
      "|  2|  Henrieta Fittes| 34|    1.72|       39|       4|             Teacher|         Master|    44|            Illinois|\n",
      "|  3|   Peyton Dulanty| 24|     1.8|       47|       5|Senior Quality En...|            PhD|    44|      North Carolina|\n",
      "|  4|     Denna Morgen| 48|    1.81|       71|       5|   Account Executive|         Master|    81|          California|\n",
      "|  5|    Camella Izaks| 34|    1.65|       60|       1|   Director of Sales|            PhD|    76|                Ohio|\n",
      "|  6|     Shara Esposi| 49|     1.5|       52|       2|     Sales Associate|         Master|    68|            Virginia|\n",
      "|  7|Saunderson Gudgen| 64|    1.93|       47|       0|Sales Representative|         Master|    85|            New York|\n",
      "|  8|Aldrich Rosendorf| 42|    1.77|       66|       0|        Developer II|            PhD|    84|District of Columbia|\n",
      "|  9| Edlin Washington| 50|    1.74|       52|       4|     Design Engineer|         Master|    88|              Oregon|\n",
      "| 10| Anders Penwright| 55|    1.98|       90|       2|     Project Manager|       Bachelor|   116|             Georgia|\n",
      "| 11|   Ruthie Cubbini| 67|    1.52|       55|       4|     Data Coordiator|            PhD|    51|District of Columbia|\n",
      "| 12|  Juliet Harbidge| 66|    1.61|       49|       2|             Actuary|            PhD|    66|         Mississippi|\n",
      "| 13|  Jeannine Kelner| 30|     1.8|       81|       1|       Programmer II|       Bachelor|    90|              Hawaii|\n",
      "| 14|  Roi Gristhwaite| 66|    1.86|       90|       1|    Product Engineer|       Bachelor|    40|           Tennessee|\n",
      "| 15|      Brant Stark| 58|    1.94|       53|       4|Sales Representative|       Bachelor|    96|      North Carolina|\n",
      "| 16|   Donny Rankling| 26|    1.51|       39|       1|Physical Therapy ...|       Bachelor|   116|            Virginia|\n",
      "| 17|     Dov Gavaghan| 27|    1.58|       68|       5|Computer Systems ...|       Bachelor|    74|                Utah|\n",
      "| 18|    Rosanne Wreak| 37|    1.76|       81|       2|Payment Adjustmen...|         Master|   103|             Florida|\n",
      "| 19|    Belva Spraggs| 50|    1.65|       80|       2|             Teacher|       Bachelor|    46|             Georgia|\n",
      "| 20|      Welch Lease| 21|    1.57|       79|       4|Mechanical System...|         Master|   114|       Massachusetts|\n",
      "+---+-----------------+---+--------+---------+--------+--------------------+---------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/1/demographics.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"demographics.csv\"), inferSchema=True, sep=',')\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- height_m: double (nullable = true)\n",
      " |-- weight_kg: integer (nullable = true)\n",
      " |-- children: integer (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- academic_degree: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will have to convert the data types of the following columns when you use Colab.\n",
    "# from pyspark.sql.types import IntegerType, DoubleType\n",
    "# df = df.withColumn(\"id\", df[\"id\"].cast(IntegerType()))\n",
    "# df = df.withColumn(\"age\", df[\"age\"].cast(IntegerType()))\n",
    "# df = df.withColumn(\"weight_kg\", df[\"weight_kg\"].cast(IntegerType()))\n",
    "# df = df.withColumn(\"children\", df[\"children\"].cast(IntegerType()))\n",
    "# df = df.withColumn(\"salary\", df[\"salary\"].cast(IntegerType()))\n",
    "# df = df.withColumn(\"height_m\", df[\"height_m\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oLG3XBTinzt",
    "outputId": "5f5b7182-7d40-44d7-f86a-957ab6824ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|       occupation|Salary|\n",
      "+-----------------+------+\n",
      "|Chemical Engineer|   120|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What occupation had the highest salary?\n",
    "df.orderBy(df[\"Salary\"].desc()).select(\"occupation\", \"Salary\").limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QW_-igVminzx",
    "outputId": "670961e2-d3fd-484d-e749-09a48378430f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|      occupation|Salary|\n",
      "+----------------+------+\n",
      "|Product Engineer|    40|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What occupation had the lowest salary?\n",
    "df.orderBy(df[\"Salary\"]).select(\"occupation\", \"Salary\").limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMhoscrSinz1",
    "outputId": "06f49816-3ec2-4be7-b40b-b6711798cc9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(Salary)|\n",
      "+-----------+\n",
      "|     79.475|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the mean salary of this dataset?\n",
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2sxkEBpinz5",
    "outputId": "464b69d6-53b8-458a-bd30-196426aa7102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Salary)|min(Salary)|\n",
      "+-----------+-----------+\n",
      "|        120|         40|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the max and min of the Salary column?\n",
    "from pyspark.sql.functions import max, min\n",
    "df.select(max(\"Salary\"), min(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q76ZPaPuinz9",
    "outputId": "8ae410f7-2810-48e4-9af5-177f29a7e309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          occupation|\n",
      "+--------------------+\n",
      "|   Account Executive|\n",
      "|Sales Representative|\n",
      "|        Developer II|\n",
      "|     Design Engineer|\n",
      "|     Project Manager|\n",
      "|       Programmer II|\n",
      "|Sales Representative|\n",
      "|Physical Therapy ...|\n",
      "|Payment Adjustmen...|\n",
      "|Mechanical System...|\n",
      "|     Media Manager I|\n",
      "|   Account Executive|\n",
      "|           Professor|\n",
      "|Community Outreac...|\n",
      "| Clinical Specialist|\n",
      "|Human Resources A...|\n",
      "|Nuclear Power Eng...|\n",
      "|      Civil Engineer|\n",
      "|Human Resources M...|\n",
      "|Senior Cost Accou...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all of the occupations where salaries were above 80k\n",
    "from pyspark.sql.functions import count\n",
    "df.filter(\"Salary > 80\").select(\"occupation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRL8cekFin0C",
    "outputId": "3e92fff0-2014-4dc4-a182-ae3c024f7388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+------------------+\n",
      "|academic_degree|          avg(age)|     avg(height_m)|\n",
      "+---------------+------------------+------------------+\n",
      "|            PhD| 42.87818696883853|1.7537393767705372|\n",
      "|         Master|42.105095541401276|1.7606050955414014|\n",
      "|       Bachelor| 43.85585585585586|1.7371771771771771|\n",
      "+---------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BONUS\n",
    "# What is the average age and height for each academic degree type?\n",
    "# HINT: You will need to use `groupby` to solve this\n",
    "avg_df = df.groupBy(\"academic_degree\").avg()\n",
    "avg_df.select(\"academic_degree\", \"avg(age)\", \"avg(height_m)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Class Objectives\n",
    "\n",
    "* rate your understanding using 1-5 method in each objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"22.1-Big-Data-Introduction to Big Data\"\n",
    "objectives = [\n",
    "    \"Identify the parts of the Hadoop ecosystem\",\n",
    "    \"Write a Python script that implements the MapReduce programming model\",\n",
    "    \"Identify the differences between the Hadoop and Spark environments\",\n",
    "    \"Create a DataFrame by using PySpark\",\n",
    "    \"Filter and order a DataFrame by using Spark\",\n",
    "]\n",
    "rating = []\n",
    "total = 0\n",
    "for i in range(len(objectives)):\n",
    "    rate = input(objectives[i]+\"? \")\n",
    "    total += int(rate)\n",
    "    rating.append(objectives[i] + \". (\" + rate + \"/5)\")\n",
    "print(\"=\"*96)\n",
    "print(f\"Self Evaluation for: {title}\")\n",
    "print(\"-\"*24)\n",
    "for i in rating:\n",
    "    print(i)\n",
    "print(\"-\"*64)\n",
    "print(\"Average: \" + str(total/len(objectives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataClass]",
   "language": "python",
   "name": "conda-env-DataClass-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
